A presente pesquisa buscou ampliar o entendimento sobre a interpretabilidade de modelos de redes neurais, 
explorando métodos que permitam elucidar as predições desses modelos complexos. A comparação sistemática 
entre um modelo de redes neurais e a tradicional regressão logística foi central para a análise, devido à natureza
interpretativa já conhecida do modelo logístico.

A utilização da técnica SHAP (SHapley Additive exPlanations) para a interpretação do modelo de redes neurais 
revelou-se uma ferramenta poderosa, permitindo uma compreensão mais profunda das contribuições de cada variável
nas predições do modelo. Este método proporcionou uma visão holística, destacando as características individuais
que mais influenciaram nas decisões do modelo.

Ao confrontar os resultados preditivos da rede neural com a regressão logística, observou-se que o modelo de redes neurais
apresentou resultados melhores na maioria das métricas, como Precisão, F1-score e tendo um desempenho abaixo apenas no Recall da
classe ``1". Esses resultados indicam que devido à arquitetura menos robusta do modelo logístico, o mesmo não conseguiu lidar
com o desbalanceamento dos dados.

Essa pesquisa contribui para a discussão em torno da interpretabilidade em inteligência
artificial e fornecendo resultados valiosos para a aplicação prática desses modelos em contextos onde a transparência é 
essencial. A busca contínua por métodos interpretativos robustos é vital para a implementação responsável e eficaz 
de modelos de aprendizado de máquina em diversos domínios.